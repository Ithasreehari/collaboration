gsutil mb gs://itha-bucket/

gsutil cp C:\Users\saisr\OneDrive\Desktop\project1\csv_files/*.csv gs://itha-bucket/scripts/

gsutil cp "C:\Users\saisr\OneDrive\Desktop\project1\dataproc_clean_merge.py" gs://itha-bucket/data/

gcloud dataproc clusters create cluster-name --region=us-central1 --zone=us-central1-a --single-node --master-machine-type=n1-standard-2 --image-version=2.0-debian10

gcloud dataproc jobs submit pyspark gs://itha-bucket/data/dataproc_clean_merge.py --cluster=cluster-name --region=us-central1


=create sql instance in that vpn network go to instance in that left vpc network ip address  paste it in MySQL
=create in instance user with username:admin and bd name with project1 and later goto connection and collect ipadress and paste in python file

gsutil cp "C:\Users\saisr\OneDrive\Desktop\project1\dataproc_failed_filter_to_mysql.py" gs://itha-bucket/data/

gcloud dataproc jobs submit pyspark gs://itha-bucket/data/dataproc_failed_filter_to_mysql.py --cluster=cluster-name --region=us-central1

=create dataset:ithadataset1 after adddata there MySQL federatedcloud then connection:itha-connection-1 then below paste id from intsabnce then database:project1 and pass:admin


CREATE OR REPLACE TABLE ithaproj1.ithadataset1.failed_transactions AS
SELECT *
FROM EXTERNAL_QUERY(
  "ithaproj1.us-central1.itha-connection-1",
  "SELECT * FROM project1.failed_transactions"
);
select * from ithadataset1.failed_transactions;





=================================================================
C:\Users\saisr\AppData\Local\Google\Cloud SDK>gsutil mb gs://itha-bucket/
Creating gs://itha-bucket/...

C:\Users\saisr\AppData\Local\Google\Cloud SDK>gsutil cp C:\Users\saisr\OneDrive\Desktop\project1\csv_files/*.csv gs://bucket-name/scripts/
Copying file://C:\Users\saisr\OneDrive\Desktop\project1\csv_files\BAN01_Bangalore_transactions.csv [Content-Type=application/vnd.ms-excel]...
AccessDeniedException: 403 saisrihari246@gmail.com does not have storage.objects.create access to the Google Cloud Storage object. Permission 'storage.objects.create' denied on resource (or it may not exist).

C:\Users\saisr\AppData\Local\Google\Cloud SDK>gsutil cp C:\Users\saisr\OneDrive\Desktop\project1\csv_files/*.csv gs://itha-bucket/scripts/
Copying file://C:\Users\saisr\OneDrive\Desktop\project1\csv_files\BAN01_Bangalore_transactions.csv [Content-Type=application/vnd.ms-excel]...
Copying file://C:\Users\saisr\OneDrive\Desktop\project1\csv_files\BAN02_Bangalore_transactions.csv [Content-Type=application/vnd.ms-excel]...
Copying file://C:\Users\saisr\OneDrive\Desktop\project1\csv_files\BAN03_Bangalore_transactions.csv [Content-Type=application/vnd.ms-excel]...
Copying file://C:\Users\saisr\OneDrive\Desktop\project1\csv_files\BAN04_Bangalore_transactions.csv [Content-Type=application/vnd.ms-excel]...
/ [4 files][ 39.4 KiB/ 39.4 KiB]
==> NOTE: You are performing a sequence of gsutil operations that may
run significantly faster if you instead use gsutil -m cp ... Please
see the -m section under "gsutil help options" for further information
about when gsutil -m can be advantageous.

Copying file://C:\Users\saisr\OneDrive\Desktop\project1\csv_files\BAN05_Bangalore_transactions.csv [Content-Type=application/vnd.ms-excel]...
Copying file://C:\Users\saisr\OneDrive\Desktop\project1\csv_files\DEL01_Delhi_transactions.csv [Content-Type=application/vnd.ms-excel]...
Copying file://C:\Users\saisr\OneDrive\Desktop\project1\csv_files\DEL02_Delhi_transactions.csv [Content-Type=application/vnd.ms-excel]...
Copying file://C:\Users\saisr\OneDrive\Desktop\project1\csv_files\DEL03_Delhi_transactions.csv [Content-Type=application/vnd.ms-excel]...
Copying file://C:\Users\saisr\OneDrive\Desktop\project1\csv_files\DEL04_Delhi_transactions.csv [Content-Type=application/vnd.ms-excel]...
Copying file://C:\Users\saisr\OneDrive\Desktop\project1\csv_files\DEL05_Delhi_transactions.csv [Content-Type=application/vnd.ms-excel]...
Copying file://C:\Users\saisr\OneDrive\Desktop\project1\csv_files\MUM01_Mumbai_transactions.csv [Content-Type=application/vnd.ms-excel]...
Copying file://C:\Users\saisr\OneDrive\Desktop\project1\csv_files\MUM02_Mumbai_transactions.csv [Content-Type=application/vnd.ms-excel]...
Copying file://C:\Users\saisr\OneDrive\Desktop\project1\csv_files\MUM03_Mumbai_transactions.csv [Content-Type=application/vnd.ms-excel]...
Copying file://C:\Users\saisr\OneDrive\Desktop\project1\csv_files\MUM04_Mumbai_transactions.csv [Content-Type=application/vnd.ms-excel]...
Copying file://C:\Users\saisr\OneDrive\Desktop\project1\csv_files\MUM05_Mumbai_transactions.csv [Content-Type=application/vnd.ms-excel]...
/ [15 files][144.6 KiB/144.6 KiB]    6.6 KiB/s
Operation completed over 15 objects/144.6 KiB.

C:\Users\saisr\AppData\Local\Google\Cloud SDK>gsutil cp "C:\Users\saisr\OneDrive\Desktop\project1\dataproc_clean_merge.py" gs://itha-bucket/data/
Copying file://C:\Users\saisr\OneDrive\Desktop\project1\dataproc_clean_merge.py [Content-Type=text/x-python]...
- [1 files][  491.0 B/  491.0 B]
Operation completed over 1 objects/491.0 B.

C:\Users\saisr\AppData\Local\Google\Cloud SDK>gcloud dataproc clusters create cluster-name --region=us-central1 --zone=us-central1-a --single-node --master- machine-type=n1-standard-2 --image-version=2.0-debian10
ERROR: (gcloud.dataproc.clusters.create) unrecognized arguments:
  --master- (did you mean '--num-masters'?)
  machine-type=n1-standard-2 (did you mean '--worker-machine-type'?)
  To search the help text of gcloud commands, run:
  gcloud help -- SEARCH_TERMS

C:\Users\saisr\AppData\Local\Google\Cloud SDK>gcloud dataproc clusters create cluster-name --region=us-central1 --zone=us-central1-a --single-node --master-machine-type=n1-standard-2 --image-version=2.0-debian10
Waiting on operation [projects/ithaproj1/regions/us-central1/operations/2473498d-71cc-3265-a4b8-bb7bb83176cc].
Waiting for cluster creation operation...working..
WARNING: Consider using Auto Zone rather than selecting a zone manually. See https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone
WARNING: Permissions are missing for the default service account '746581037773-compute@developer.gserviceaccount.com', missing permissions: [storage.objects.update] on the project 'projects/ithaproj1'. This usually happens when a custom resource (ex: custom staging bucket) or a user-managed VM Service account has been provided and the default/user-managed service account hasn't been granted enough permissions on the resource. See https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/service-accounts#VM_service_account.
WARNING: The firewall rules for specified network or subnetwork would allow ingress traffic from 0.0.0.0/0, which could be a security risk.
WARNING: Unable to validate the staging bucket lifecycle configuration of the bucket 'dataproc-staging-us-central1-746581037773-yalthsq2' due to an internal error, Please make sure that the provided bucket doesn't have any delete rules set.
Waiting for cluster creation operation...done.
Created [https://dataproc.googleapis.com/v1/projects/ithaproj1/regions/us-central1/clusters/cluster-name] Cluster placed in zone [us-central1-a].

C:\Users\saisr\AppData\Local\Google\Cloud SDK>gcloud dataproc jobs submit pyspark gs://itha-bucket/data/dataproc_clean_merge.py --cluster=cluster-name --region=us-central1
Job [f5f57a1f7c8c45deb36ace97bedf800a] submitted.
Waiting for job output...
25/05/08 16:12:06 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
25/05/08 16:12:06 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
25/05/08 16:12:06 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat
25/05/08 16:12:06 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
25/05/08 16:12:06 INFO org.sparkproject.jetty.util.log: Logging initialized @5427ms to org.sparkproject.jetty.util.log.Slf4jLog
25/05/08 16:12:06 INFO org.sparkproject.jetty.server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_452-b09
25/05/08 16:12:06 INFO org.sparkproject.jetty.server.Server: Started @5579ms
25/05/08 16:12:06 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@7bdf4580{HTTP/1.1, (http/1.1)}{0.0.0.0:37843}
25/05/08 16:12:09 INFO com.google.cloud.dataproc.DataprocSparkPlugin: Registered 128 driver metrics
25/05/08 16:12:10 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at cluster-name-m/10.128.0.14:8032
25/05/08 16:12:10 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at cluster-name-m/10.128.0.14:10200
25/05/08 16:12:11 INFO org.apache.hadoop.conf.Configuration: resource-types.xml not found
25/05/08 16:12:11 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Unable to find 'resource-types.xml'.
25/05/08 16:12:13 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1746720119508_0001
25/05/08 16:12:14 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at cluster-name-m/10.128.0.14:8030
25/05/08 16:12:17 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
Traceback (most recent call last):
  File "/tmp/f5f57a1f7c8c45deb36ace97bedf800a/dataproc_clean_merge.py", line 6, in <module>
    df = spark.read.option("header", "true").csv("gs://itha-bucket/script/*.csv")
  File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 737, in csv
  File "/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1304, in __call__
  File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in deco
pyspark.sql.utils.AnalysisException: Path does not exist: gs://itha-bucket/script/*.csv
25/05/08 16:12:23 INFO org.sparkproject.jetty.server.AbstractConnector: Stopped Spark@7bdf4580{HTTP/1.1, (http/1.1)}{0.0.0.0:0}
25/05/08 16:12:24 INFO com.google.cloud.dataproc.DataprocSparkPlugin: Shutting down driver plugin. metrics=[files_created=1, gcs_api_server_not_implemented_error_count=0, gcs_api_server_timeout_count=0, action_http_post_request_failures=0, op_get_list_status_result_size=0, op_open=0, gcs_api_client_unauthorized_response_count=0, action_http_head_request_failures=0, stream_read_close_operations=0, stream_read_bytes_backwards_on_seek=0, exception_count=18, gcs_api_total_request_count=16, op_create=1, gcs_api_client_bad_request_count=0, op_create_non_recursive=0, gcs_api_client_gone_response_count=0, stream_write_operations=0, stream_read_operations=0, gcs_api_client_request_timeout_count=0, op_rename=0, op_get_file_status=2, stream_read_total_bytes=0, op_glob_status=1, stream_read_exceptions=0, action_http_get_request_failures=0, op_exists=0, stream_write_bytes=88616, op_xattr_list=0, stream_write_exceptions=0, gcs_api_server_unavailable_count=0, directories_created=0, files_delete_rejected=0, op_xattr_get_named=0, op_hsync=0, stream_read_operations_incomplete=0, op_delete=0, stream_read_bytes=0, gcs_api_client_non_found_response_count=8, gcs_api_client_requested_range_not_statisfiable_count=0, op_hflush=0, op_list_status=1, op_xattr_get_named_map=0, gcs_api_client_side_error_count=24, op_get_file_checksum=0, action_http_delete_request_failures=0, gcs_api_server_internal_error_count=0, stream_read_seek_bytes_skipped=0, stream_write_close_operations=0, op_list_files=0, files_deleted=0, op_mkdirs=1, gcs_api_client_rate_limit_error_count=0, action_http_put_request_failures=0, gcs_api_server_bad_gateway_count=0, stream_read_seek_backward_operations=0, gcs_api_server_side_error_count=0, action_http_patch_request_failures=0, stream_read_seek_operations=0, stream_read_seek_forward_operations=0, gcs_api_client_precondition_failed_response_count=1, directories_deleted=0, op_xattr_get_map=0, delegation_tokens_issued=0, op_create_min=170, op_delete_min=0, op_mkdirs_min=457, op_create_non_recursive_min=0, op_glob_status_min=167, op_hsync_min=0, op_xattr_get_named_min=0, op_list_status_min=50, op_xattr_get_named_map_min=0, stream_read_close_operations_min=0, stream_read_operations_min=0, stream_read_seek_operations_min=0, op_hflush_min=0, op_xattr_get_map_min=0, op_xattr_list_min=0, stream_write_operations_min=0, op_get_file_status_min=105, op_open_min=0, op_rename_min=0, delegation_tokens_issued_min=0, stream_write_close_operations_min=0, stream_read_close_operations_max=0, stream_read_operations_max=0, stream_read_seek_operations_max=0, op_hflush_max=0, op_xattr_list_max=0, op_xattr_get_map_max=0, op_xattr_get_named_max=0, op_create_non_recursive_max=0, op_glob_status_max=167, op_get_file_status_max=498, stream_write_close_operations_max=0, op_open_max=0, delegation_tokens_issued_max=0, op_mkdirs_max=457, op_rename_max=0, op_create_max=170, op_delete_max=0, op_list_status_max=50, op_xattr_get_named_map_max=0, stream_write_operations_max=0, op_hsync_max=0, op_list_status_mean=50, stream_read_close_operations_mean=0, op_open_mean=0, op_xattr_get_named_map_mean=0, op_xattr_list_mean=0, op_mkdirs_mean=457, stream_write_close_operations_mean=0, op_rename_mean=0, op_hsync_mean=0, delegation_tokens_issued_mean=0, stream_read_operations_mean=0, op_xattr_get_map_mean=0, op_create_mean=170, op_glob_status_mean=167, op_delete_mean=0, stream_read_seek_operations_mean=0, stream_write_operations_mean=0, op_create_non_recursive_mean=0, op_hflush_mean=0, op_xattr_get_named_mean=0, op_get_file_status_mean=301, stream_write_operations_duration=0, stream_read_operations_duration=0]
ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [f5f57a1f7c8c45deb36ace97bedf800a] failed with error:
Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found at:
https://console.cloud.google.com/dataproc/jobs/f5f57a1f7c8c45deb36ace97bedf800a?project=ithaproj1&region=us-central1
gcloud dataproc jobs wait 'f5f57a1f7c8c45deb36ace97bedf800a' --region 'us-central1' --project 'ithaproj1'
https://console.cloud.google.com/storage/browser/dataproc-staging-us-central1-746581037773-yalthsq2/google-cloud-dataproc-metainfo/4b454d3c-eb6c-4539-ad8d-85da9186474b/jobs/f5f57a1f7c8c45deb36ace97bedf800a/
gs://dataproc-staging-us-central1-746581037773-yalthsq2/google-cloud-dataproc-metainfo/4b454d3c-eb6c-4539-ad8d-85da9186474b/jobs/f5f57a1f7c8c45deb36ace97bedf800a/driveroutput.*

C:\Users\saisr\AppData\Local\Google\Cloud SDK>gsutil cp "C:\Users\saisr\OneDrive\Desktop\project1\dataproc_clean_merge.py" gs://itha-bucket/data/
Copying file://C:\Users\saisr\OneDrive\Desktop\project1\dataproc_clean_merge.py [Content-Type=text/x-python]...
- [1 files][  397.0 B/  397.0 B]
Operation completed over 1 objects/397.0 B.

C:\Users\saisr\AppData\Local\Google\Cloud SDK>gcloud dataproc jobs submit pyspark gs://itha-bucket/data/dataproc_failed_filter_to_mysql.py --cluster=cluster-name --region=us-central1
Job [ae9279fc345f45e8b6db56bd2bb1ed18] submitted.
Waiting for job output...
=========== Cloud Dataproc Agent Error ===========
java.io.FileNotFoundException: File not found: gs://itha-bucket/data/dataproc_failed_filter_to_mysql.py
        at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.lambda$getFileStatus$10(GoogleHadoopFileSystemBase.java:1088)
        at com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics.trackDuration(GhfsStorageStatistics.java:116)
        at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getFileStatus(GoogleHadoopFileSystemBase.java:1073)
        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:392)
        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:343)
        at org.apache.hadoop.fs.FileSystem.copyToLocalFile(FileSystem.java:2464)
        at org.apache.hadoop.fs.FileSystem.copyToLocalFile(FileSystem.java:2433)
        at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.copyToLocalFile(GoogleHadoopFileSystemBase.java:1760)
        at org.apache.hadoop.fs.FileSystem.copyToLocalFile(FileSystem.java:2409)
        at com.google.cloud.hadoop.services.agent.util.HadoopUtil.download(HadoopUtil.java:165)
        at com.google.cloud.hadoop.services.agent.job.handler.AbstractJobHandler.downloadResources(AbstractJobHandler.java:621)
        at com.google.cloud.hadoop.services.agent.job.handler.AbstractJobHandler.prepareDriver(AbstractJobHandler.java:927)
        at com.google.cloud.hadoop.services.repackaged.com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:131)
        at com.google.cloud.hadoop.services.repackaged.com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:76)
        at com.google.cloud.hadoop.services.repackaged.com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:82)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
======== End of Cloud Dataproc Agent Error ========
ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [ae9279fc345f45e8b6db56bd2bb1ed18] failed with error:
File not found: gs://itha-bucket/data/dataproc_failed_filter_to_mysql.py

C:\Users\saisr\AppData\Local\Google\Cloud SDK>gcloud dataproc jobs submit pyspark gs://itha-bucket/data/dataproc_clean_merge.py --cluster=cluster-name --region=us-central1
Job [176809078e7542afbe34128970d68085] submitted.
Waiting for job output...
25/05/08 16:26:55 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
25/05/08 16:26:55 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
25/05/08 16:26:55 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat
25/05/08 16:26:56 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
25/05/08 16:26:56 INFO org.sparkproject.jetty.util.log: Logging initialized @4799ms to org.sparkproject.jetty.util.log.Slf4jLog
25/05/08 16:26:56 INFO org.sparkproject.jetty.server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_452-b09
25/05/08 16:26:56 INFO org.sparkproject.jetty.server.Server: Started @4976ms
25/05/08 16:26:56 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@2e903cc0{HTTP/1.1, (http/1.1)}{0.0.0.0:41953}
25/05/08 16:26:58 INFO com.google.cloud.dataproc.DataprocSparkPlugin: Registered 128 driver metrics
25/05/08 16:26:59 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at cluster-name-m/10.128.0.14:8032
25/05/08 16:27:00 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at cluster-name-m/10.128.0.14:10200
25/05/08 16:27:00 INFO org.apache.hadoop.conf.Configuration: resource-types.xml not found
25/05/08 16:27:00 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Unable to find 'resource-types.xml'.
25/05/08 16:27:02 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1746720119508_0002
25/05/08 16:27:03 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at cluster-name-m/10.128.0.14:8030
25/05/08 16:27:05 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
25/05/08 16:27:53 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://itha-bucket/processed/cleaned_transactions.csv/' directory.
25/05/08 16:27:53 INFO org.sparkproject.jetty.server.AbstractConnector: Stopped Spark@2e903cc0{HTTP/1.1, (http/1.1)}{0.0.0.0:0}
25/05/08 16:27:54 INFO com.google.cloud.dataproc.DataprocSparkPlugin: Shutting down driver plugin. metrics=[files_created=2, gcs_api_server_not_implemented_error_count=0, gcs_api_server_timeout_count=0, action_http_post_request_failures=0, op_get_list_status_result_size=45, op_open=0, gcs_api_client_unauthorized_response_count=0, action_http_head_request_failures=0, stream_read_close_operations=0, stream_read_bytes_backwards_on_seek=0, exception_count=35, gcs_api_total_request_count=98, op_create=2, gcs_api_client_bad_request_count=0, op_create_non_recursive=0, gcs_api_client_gone_response_count=0, stream_write_operations=0, stream_read_operations=0, gcs_api_client_request_timeout_count=0, op_rename=0, op_get_file_status=3, stream_read_total_bytes=0, op_glob_status=1, stream_read_exceptions=0, action_http_get_request_failures=0, op_exists=0, stream_write_bytes=165454, op_xattr_list=0, stream_write_exceptions=0, gcs_api_server_unavailable_count=0, directories_created=0, files_delete_rejected=0, op_xattr_get_named=0, op_hsync=0, stream_read_operations_incomplete=0, op_delete=2, stream_read_bytes=0, gcs_api_client_non_found_response_count=19, gcs_api_client_requested_range_not_statisfiable_count=0, op_hflush=0, op_list_status=31, op_xattr_get_named_map=0, gcs_api_client_side_error_count=52, op_get_file_checksum=0, action_http_delete_request_failures=0, gcs_api_server_internal_error_count=0, stream_read_seek_bytes_skipped=0, stream_write_close_operations=1, op_list_files=0, files_deleted=0, op_mkdirs=2, gcs_api_client_rate_limit_error_count=0, action_http_put_request_failures=0, gcs_api_server_bad_gateway_count=0, stream_read_seek_backward_operations=0, gcs_api_server_side_error_count=0, action_http_patch_request_failures=0, stream_read_seek_operations=0, stream_read_seek_forward_operations=0, gcs_api_client_precondition_failed_response_count=1, directories_deleted=0, op_xattr_get_map=0, delegation_tokens_issued=0, op_create_min=88, op_delete_min=30, op_mkdirs_min=148, op_create_non_recursive_min=0, op_glob_status_min=364, op_hsync_min=0, op_xattr_get_named_min=0, op_list_status_min=16, op_xattr_get_named_map_min=0, stream_read_close_operations_min=0, stream_read_operations_min=0, stream_read_seek_operations_min=0, op_hflush_min=0, op_xattr_get_map_min=0, op_xattr_list_min=0, stream_write_operations_min=0, op_get_file_status_min=96, op_open_min=0, op_rename_min=0, delegation_tokens_issued_min=0, stream_write_close_operations_min=131, stream_read_close_operations_max=0, stream_read_operations_max=0, stream_read_seek_operations_max=0, op_hflush_max=0, op_xattr_list_max=0, op_xattr_get_map_max=0, op_xattr_get_named_max=0, op_create_non_recursive_max=0, op_glob_status_max=364, op_get_file_status_max=356, stream_write_close_operations_max=131, op_open_max=0, delegation_tokens_issued_max=0, op_mkdirs_max=308, op_rename_max=0, op_create_max=110, op_delete_max=239, op_list_status_max=84, op_xattr_get_named_map_max=0, stream_write_operations_max=0, op_hsync_max=0, op_list_status_mean=33, stream_read_close_operations_mean=0, op_open_mean=0, op_xattr_get_named_map_mean=0, op_xattr_list_mean=0, op_mkdirs_mean=228, stream_write_close_operations_mean=131, op_rename_mean=0, op_hsync_mean=0, delegation_tokens_issued_mean=0, stream_read_operations_mean=0, op_xattr_get_map_mean=0, op_create_mean=99, op_glob_status_mean=364, op_delete_mean=134, stream_read_seek_operations_mean=0, stream_write_operations_mean=0, op_create_non_recursive_mean=0, op_hflush_mean=0, op_xattr_get_named_mean=0, op_get_file_status_mean=193, stream_write_operations_duration=0, stream_read_operations_duration=0]
Job [176809078e7542afbe34128970d68085] finished successfully.
done: true
driverControlFilesUri: gs://dataproc-staging-us-central1-746581037773-yalthsq2/google-cloud-dataproc-metainfo/4b454d3c-eb6c-4539-ad8d-85da9186474b/jobs/176809078e7542afbe34128970d68085/
driverOutputResourceUri: gs://dataproc-staging-us-central1-746581037773-yalthsq2/google-cloud-dataproc-metainfo/4b454d3c-eb6c-4539-ad8d-85da9186474b/jobs/176809078e7542afbe34128970d68085/driveroutput
jobUuid: d95228d6-8d6d-344a-a2f4-41722314b1f9
placement:
  clusterName: cluster-name
  clusterUuid: 4b454d3c-eb6c-4539-ad8d-85da9186474b
pysparkJob:
  mainPythonFileUri: gs://itha-bucket/data/dataproc_clean_merge.py
reference:
  jobId: 176809078e7542afbe34128970d68085
  projectId: ithaproj1
status:
  state: DONE
  stateStartTime: '2025-05-08T16:27:55.394577Z'
statusHistory:
- state: PENDING
  stateStartTime: '2025-05-08T16:26:50.290699Z'
- state: SETUP_DONE
  stateStartTime: '2025-05-08T16:26:50.322129Z'
- details: Agent reported job success
  state: RUNNING
  stateStartTime: '2025-05-08T16:26:50.523648Z'
yarnApplications:
- name: CleanAndMergeTransactions
  progress: 1.0
  state: FINISHED
  trackingUrl: http://cluster-name-m:8088/proxy/application_1746720119508_0002/

C:\Users\saisr\AppData\Local\Google\Cloud SDK>gsutil cp "C:\Users\saisr\OneDrive\Desktop\project1\dataproc_failed_filter_to_mysql.py" gs://itha-bucket/data/
Copying file://C:\Users\saisr\OneDrive\Desktop\project1\dataproc_failed_filter_to_mysql.py [Content-Type=text/x-python]...
- [1 files][  648.0 B/  648.0 B]
Operation completed over 1 objects/648.0 B.

C:\Users\saisr\AppData\Local\Google\Cloud SDK>gcloud dataproc jobs submit pyspark gs://itha-bucket/data/dataproc_failed_filter_to_mysql.py --cluster=cluster-name --region=us-central1
Job [c78c8e71acd1431ca501ad91f16f9918] submitted.
Waiting for job output...
25/05/08 16:33:14 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
25/05/08 16:33:14 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
25/05/08 16:33:14 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat
25/05/08 16:33:14 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
25/05/08 16:33:14 INFO org.sparkproject.jetty.util.log: Logging initialized @5095ms to org.sparkproject.jetty.util.log.Slf4jLog
25/05/08 16:33:14 INFO org.sparkproject.jetty.server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_452-b09
25/05/08 16:33:14 INFO org.sparkproject.jetty.server.Server: Started @5296ms
25/05/08 16:33:15 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@1e327e76{HTTP/1.1, (http/1.1)}{0.0.0.0:38053}
25/05/08 16:33:17 INFO com.google.cloud.dataproc.DataprocSparkPlugin: Registered 128 driver metrics
25/05/08 16:33:18 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at cluster-name-m/10.128.0.14:8032
25/05/08 16:33:18 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at cluster-name-m/10.128.0.14:10200
25/05/08 16:33:19 INFO org.apache.hadoop.conf.Configuration: resource-types.xml not found
25/05/08 16:33:19 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Unable to find 'resource-types.xml'.
25/05/08 16:33:20 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1746720119508_0003
25/05/08 16:33:21 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at cluster-name-m/10.128.0.14:8030
25/05/08 16:33:23 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
Traceback (most recent call last):
  File "/tmp/c78c8e71acd1431ca501ad91f16f9918/dataproc_failed_filter_to_mysql.py", line 19, in <module>
    df_failed.write.jdbc(url=jdbc_url, table="failed_transactions", mode="overwrite", properties=properties)
  File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 1445, in jdbc
  File "/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1304, in __call__
  File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 111, in deco
  File "/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py", line 326, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o74.jdbc.
: java.sql.SQLException: Access denied for user 'admin'@'34.31.216.204' (using password: YES)
        at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:129)
        at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)
        at com.mysql.cj.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:824)
        at com.mysql.cj.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:444)
        at com.mysql.cj.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:237)
        at com.mysql.cj.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:198)
        at org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)
        at org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProvider$.create(ConnectionProvider.scala:77)
        at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$createConnectionFactory$1(JdbcUtils.scala:62)
        at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:48)
        at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)
        at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
        at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
        at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
        at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:133)
        at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:132)
        at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
        at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)
        at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)
        at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)
        at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:817)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:238)
        at java.lang.Thread.run(Thread.java:750)

25/05/08 16:33:53 INFO org.sparkproject.jetty.server.AbstractConnector: Stopped Spark@1e327e76{HTTP/1.1, (http/1.1)}{0.0.0.0:0}
25/05/08 16:33:53 INFO com.google.cloud.dataproc.DataprocSparkPlugin: Shutting down driver plugin. metrics=[files_created=1, gcs_api_server_not_implemented_error_count=0, gcs_api_server_timeout_count=0, action_http_post_request_failures=0, op_get_list_status_result_size=7, op_open=0, gcs_api_client_unauthorized_response_count=0, action_http_head_request_failures=0, stream_read_close_operations=0, stream_read_bytes_backwards_on_seek=0, exception_count=16, gcs_api_total_request_count=24, op_create=1, gcs_api_client_bad_request_count=0, op_create_non_recursive=0, gcs_api_client_gone_response_count=0, stream_write_operations=0, stream_read_operations=0, gcs_api_client_request_timeout_count=0, op_rename=0, op_get_file_status=2, stream_read_total_bytes=0, op_glob_status=1, stream_read_exceptions=0, action_http_get_request_failures=0, op_exists=0, stream_write_bytes=115372, op_xattr_list=0, stream_write_exceptions=0, gcs_api_server_unavailable_count=0, directories_created=0, files_delete_rejected=0, op_xattr_get_named=0, op_hsync=0, stream_read_operations_incomplete=0, op_delete=0, stream_read_bytes=0, gcs_api_client_non_found_response_count=8, gcs_api_client_requested_range_not_statisfiable_count=0, op_hflush=0, op_list_status=5, op_xattr_get_named_map=0, gcs_api_client_side_error_count=24, op_get_file_checksum=0, action_http_delete_request_failures=0, gcs_api_server_internal_error_count=0, stream_read_seek_bytes_skipped=0, stream_write_close_operations=0, op_list_files=0, files_deleted=0, op_mkdirs=1, gcs_api_client_rate_limit_error_count=0, action_http_put_request_failures=0, gcs_api_server_bad_gateway_count=0, stream_read_seek_backward_operations=0, gcs_api_server_side_error_count=0, action_http_patch_request_failures=0, stream_read_seek_operations=0, stream_read_seek_forward_operations=0, gcs_api_client_precondition_failed_response_count=1, directories_deleted=0, op_xattr_get_map=0, delegation_tokens_issued=0, op_create_min=161, op_delete_min=0, op_mkdirs_min=312, op_create_non_recursive_min=0, op_glob_status_min=327, op_hsync_min=0, op_xattr_get_named_min=0, op_list_status_min=39, op_xattr_get_named_map_min=0, stream_read_close_operations_min=0, stream_read_operations_min=0, stream_read_seek_operations_min=0, op_hflush_min=0, op_xattr_get_map_min=0, op_xattr_list_min=0, stream_write_operations_min=0, op_get_file_status_min=127, op_open_min=0, op_rename_min=0, delegation_tokens_issued_min=0, stream_write_close_operations_min=0, stream_read_close_operations_max=0, stream_read_operations_max=0, stream_read_seek_operations_max=0, op_hflush_max=0, op_xattr_list_max=0, op_xattr_get_map_max=0, op_xattr_get_named_max=0, op_create_non_recursive_max=0, op_glob_status_max=327, op_get_file_status_max=336, stream_write_close_operations_max=0, op_open_max=0, delegation_tokens_issued_max=0, op_mkdirs_max=312, op_rename_max=0, op_create_max=161, op_delete_max=0, op_list_status_max=77, op_xattr_get_named_map_max=0, stream_write_operations_max=0, op_hsync_max=0, op_list_status_mean=49, stream_read_close_operations_mean=0, op_open_mean=0, op_xattr_get_named_map_mean=0, op_xattr_list_mean=0, op_mkdirs_mean=312, stream_write_close_operations_mean=0, op_rename_mean=0, op_hsync_mean=0, delegation_tokens_issued_mean=0, stream_read_operations_mean=0, op_xattr_get_map_mean=0, op_create_mean=161, op_glob_status_mean=327, op_delete_mean=0, stream_read_seek_operations_mean=0, stream_write_operations_mean=0, op_create_non_recursive_mean=0, op_hflush_mean=0, op_xattr_get_named_mean=0, op_get_file_status_mean=231, stream_write_operations_duration=0, stream_read_operations_duration=0]
ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [c78c8e71acd1431ca501ad91f16f9918] failed with error:
Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found at:
https://console.cloud.google.com/dataproc/jobs/c78c8e71acd1431ca501ad91f16f9918?project=ithaproj1&region=us-central1
gcloud dataproc jobs wait 'c78c8e71acd1431ca501ad91f16f9918' --region 'us-central1' --project 'ithaproj1'
https://console.cloud.google.com/storage/browser/dataproc-staging-us-central1-746581037773-yalthsq2/google-cloud-dataproc-metainfo/4b454d3c-eb6c-4539-ad8d-85da9186474b/jobs/c78c8e71acd1431ca501ad91f16f9918/
gs://dataproc-staging-us-central1-746581037773-yalthsq2/google-cloud-dataproc-metainfo/4b454d3c-eb6c-4539-ad8d-85da9186474b/jobs/c78c8e71acd1431ca501ad91f16f9918/driveroutput.*

C:\Users\saisr\AppData\Local\Google\Cloud SDK>gcloud dataproc jobs submit pyspark gs://itha-bucket/data/dataproc_failed_filter_to_mysql.py --cluster=cluster-name --region=us-central1
Job [317a5b2900e84a228313c9aad40214ba] submitted.
Waiting for job output...
25/05/08 16:39:05 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
25/05/08 16:39:05 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
25/05/08 16:39:05 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat
25/05/08 16:39:06 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
25/05/08 16:39:06 INFO org.sparkproject.jetty.util.log: Logging initialized @4696ms to org.sparkproject.jetty.util.log.Slf4jLog
25/05/08 16:39:06 INFO org.sparkproject.jetty.server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_452-b09
25/05/08 16:39:06 INFO org.sparkproject.jetty.server.Server: Started @4853ms
25/05/08 16:39:06 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@273a1c57{HTTP/1.1, (http/1.1)}{0.0.0.0:44527}
25/05/08 16:39:09 INFO com.google.cloud.dataproc.DataprocSparkPlugin: Registered 128 driver metrics
25/05/08 16:39:09 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at cluster-name-m/10.128.0.14:8032
25/05/08 16:39:10 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at cluster-name-m/10.128.0.14:10200
25/05/08 16:39:11 INFO org.apache.hadoop.conf.Configuration: resource-types.xml not found
25/05/08 16:39:11 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Unable to find 'resource-types.xml'.
25/05/08 16:39:12 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1746720119508_0004
25/05/08 16:39:13 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at cluster-name-m/10.128.0.14:8030
25/05/08 16:39:14 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
Traceback (most recent call last):
  File "/tmp/317a5b2900e84a228313c9aad40214ba/dataproc_failed_filter_to_mysql.py", line 19, in <module>
    df_failed.write.jdbc(url=jdbc_url, table="failed_transactions", mode="overwrite", properties=properties)
  File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 1445, in jdbc
  File "/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1304, in __call__
  File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 111, in deco
  File "/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py", line 326, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o74.jdbc.
: java.sql.SQLException: Access denied for user 'admin'@'34.31.216.204' (using password: YES)
        at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:129)
        at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)
        at com.mysql.cj.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:824)
        at com.mysql.cj.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:444)
        at com.mysql.cj.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:237)
        at com.mysql.cj.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:198)
        at org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)
        at org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProvider$.create(ConnectionProvider.scala:77)
        at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$createConnectionFactory$1(JdbcUtils.scala:62)
        at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:48)
        at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)
        at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
        at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
        at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
        at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:133)
        at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:132)
        at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
        at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)
        at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)
        at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)
        at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:817)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:238)
        at java.lang.Thread.run(Thread.java:750)

25/05/08 16:39:44 INFO org.sparkproject.jetty.server.AbstractConnector: Stopped Spark@273a1c57{HTTP/1.1, (http/1.1)}{0.0.0.0:0}
25/05/08 16:39:45 INFO com.google.cloud.dataproc.DataprocSparkPlugin: Shutting down driver plugin. metrics=[files_created=1, gcs_api_server_not_implemented_error_count=0, gcs_api_server_timeout_count=0, action_http_post_request_failures=0, op_get_list_status_result_size=7, op_open=0, gcs_api_client_unauthorized_response_count=0, action_http_head_request_failures=0, stream_read_close_operations=0, stream_read_bytes_backwards_on_seek=0, exception_count=16, gcs_api_total_request_count=24, op_create=1, gcs_api_client_bad_request_count=0, op_create_non_recursive=0, gcs_api_client_gone_response_count=0, stream_write_operations=0, stream_read_operations=0, gcs_api_client_request_timeout_count=0, op_rename=0, op_get_file_status=2, stream_read_total_bytes=0, op_glob_status=1, stream_read_exceptions=0, action_http_get_request_failures=0, op_exists=0, stream_write_bytes=115372, op_xattr_list=0, stream_write_exceptions=0, gcs_api_server_unavailable_count=0, directories_created=0, files_delete_rejected=0, op_xattr_get_named=0, op_hsync=0, stream_read_operations_incomplete=0, op_delete=0, stream_read_bytes=0, gcs_api_client_non_found_response_count=8, gcs_api_client_requested_range_not_statisfiable_count=0, op_hflush=0, op_list_status=5, op_xattr_get_named_map=0, gcs_api_client_side_error_count=24, op_get_file_checksum=0, action_http_delete_request_failures=0, gcs_api_server_internal_error_count=0, stream_read_seek_bytes_skipped=0, stream_write_close_operations=0, op_list_files=0, files_deleted=0, op_mkdirs=1, gcs_api_client_rate_limit_error_count=0, action_http_put_request_failures=0, gcs_api_server_bad_gateway_count=0, stream_read_seek_backward_operations=0, gcs_api_server_side_error_count=0, action_http_patch_request_failures=0, stream_read_seek_operations=0, stream_read_seek_forward_operations=0, gcs_api_client_precondition_failed_response_count=1, directories_deleted=0, op_xattr_get_map=0, delegation_tokens_issued=0, op_create_min=185, op_delete_min=0, op_mkdirs_min=299, op_create_non_recursive_min=0, op_glob_status_min=246, op_hsync_min=0, op_xattr_get_named_min=0, op_list_status_min=28, op_xattr_get_named_map_min=0, stream_read_close_operations_min=0, stream_read_operations_min=0, stream_read_seek_operations_min=0, op_hflush_min=0, op_xattr_get_map_min=0, op_xattr_list_min=0, stream_write_operations_min=0, op_get_file_status_min=134, op_open_min=0, op_rename_min=0, delegation_tokens_issued_min=0, stream_write_close_operations_min=0, stream_read_close_operations_max=0, stream_read_operations_max=0, stream_read_seek_operations_max=0, op_hflush_max=0, op_xattr_list_max=0, op_xattr_get_map_max=0, op_xattr_get_named_max=0, op_create_non_recursive_max=0, op_glob_status_max=246, op_get_file_status_max=431, stream_write_close_operations_max=0, op_open_max=0, delegation_tokens_issued_max=0, op_mkdirs_max=299, op_rename_max=0, op_create_max=185, op_delete_max=0, op_list_status_max=65, op_xattr_get_named_map_max=0, stream_write_operations_max=0, op_hsync_max=0, op_list_status_mean=38, stream_read_close_operations_mean=0, op_open_mean=0, op_xattr_get_named_map_mean=0, op_xattr_list_mean=0, op_mkdirs_mean=299, stream_write_close_operations_mean=0, op_rename_mean=0, op_hsync_mean=0, delegation_tokens_issued_mean=0, stream_read_operations_mean=0, op_xattr_get_map_mean=0, op_create_mean=185, op_glob_status_mean=246, op_delete_mean=0, stream_read_seek_operations_mean=0, stream_write_operations_mean=0, op_create_non_recursive_mean=0, op_hflush_mean=0, op_xattr_get_named_mean=0, op_get_file_status_mean=282, stream_write_operations_duration=0, stream_read_operations_duration=0]
ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [317a5b2900e84a228313c9aad40214ba] failed with error:
Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found at:
https://console.cloud.google.com/dataproc/jobs/317a5b2900e84a228313c9aad40214ba?project=ithaproj1&region=us-central1
gcloud dataproc jobs wait '317a5b2900e84a228313c9aad40214ba' --region 'us-central1' --project 'ithaproj1'
https://console.cloud.google.com/storage/browser/dataproc-staging-us-central1-746581037773-yalthsq2/google-cloud-dataproc-metainfo/4b454d3c-eb6c-4539-ad8d-85da9186474b/jobs/317a5b2900e84a228313c9aad40214ba/
gs://dataproc-staging-us-central1-746581037773-yalthsq2/google-cloud-dataproc-metainfo/4b454d3c-eb6c-4539-ad8d-85da9186474b/jobs/317a5b2900e84a228313c9aad40214ba/driveroutput.*

C:\Users\saisr\AppData\Local\Google\Cloud SDK>gsutil cp "C:\Users\saisr\OneDrive\Desktop\project1\dataproc_failed_filter_to_mysql.py" gs://itha-bucket/data/
Copying file://C:\Users\saisr\OneDrive\Desktop\project1\dataproc_failed_filter_to_mysql.py [Content-Type=text/x-python]...
- [1 files][  648.0 B/  648.0 B]
Operation completed over 1 objects/648.0 B.

C:\Users\saisr\AppData\Local\Google\Cloud SDK>gcloud dataproc jobs submit pyspark gs://itha-bucket/data/dataproc_failed_filter_to_mysql.py --cluster=cluster-name --region=us-central1
Job [13c44a9dc0e54e499f37bf7de23d0224] submitted.
Waiting for job output...
25/05/08 16:43:12 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
25/05/08 16:43:12 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
25/05/08 16:43:13 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat
25/05/08 16:43:13 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
25/05/08 16:43:13 INFO org.sparkproject.jetty.util.log: Logging initialized @4664ms to org.sparkproject.jetty.util.log.Slf4jLog
25/05/08 16:43:13 INFO org.sparkproject.jetty.server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_452-b09
25/05/08 16:43:13 INFO org.sparkproject.jetty.server.Server: Started @4833ms
25/05/08 16:43:13 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@7bdf4580{HTTP/1.1, (http/1.1)}{0.0.0.0:45899}
25/05/08 16:43:15 INFO com.google.cloud.dataproc.DataprocSparkPlugin: Registered 128 driver metrics
25/05/08 16:43:16 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at cluster-name-m/10.128.0.14:8032
25/05/08 16:43:16 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at cluster-name-m/10.128.0.14:10200
25/05/08 16:43:17 INFO org.apache.hadoop.conf.Configuration: resource-types.xml not found
25/05/08 16:43:17 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Unable to find 'resource-types.xml'.
25/05/08 16:43:18 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1746720119508_0005
25/05/08 16:43:19 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at cluster-name-m/10.128.0.14:8030
25/05/08 16:43:21 INFO com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation op_get_file_status. latencyMs=517; previousMaxLatencyMs=0; operationCount=1; context=gs://dataproc-temp-us-central1-746581037773-8zj5ivyl/4b454d3c-eb6c-4539-ad8d-85da9186474b/spark-job-history
25/05/08 16:43:22 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
Traceback (most recent call last):
  File "/tmp/13c44a9dc0e54e499f37bf7de23d0224/dataproc_failed_filter_to_mysql.py", line 19, in <module>
    df_failed.write.jdbc(url=jdbc_url, table="failed_transactions", mode="overwrite", properties=properties)
  File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 1445, in jdbc
  File "/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1304, in __call__
  File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 111, in deco
  File "/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py", line 326, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o74.jdbc.
: java.sql.SQLException: Access denied for user 'admin'@'34.31.216.204' (using password: YES)
        at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:129)
        at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)
        at com.mysql.cj.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:824)
        at com.mysql.cj.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:444)
        at com.mysql.cj.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:237)
        at com.mysql.cj.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:198)
        at org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)
        at org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProvider$.create(ConnectionProvider.scala:77)
        at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$createConnectionFactory$1(JdbcUtils.scala:62)
        at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:48)
        at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)
        at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
        at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
        at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
        at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:133)
        at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:132)
        at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
        at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)
        at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)
        at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)
        at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:817)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:238)
        at java.lang.Thread.run(Thread.java:750)

25/05/08 16:43:48 INFO org.sparkproject.jetty.server.AbstractConnector: Stopped Spark@7bdf4580{HTTP/1.1, (http/1.1)}{0.0.0.0:0}
25/05/08 16:43:49 INFO com.google.cloud.dataproc.DataprocSparkPlugin: Shutting down driver plugin. metrics=[files_created=1, gcs_api_server_not_implemented_error_count=0, gcs_api_server_timeout_count=0, action_http_post_request_failures=0, op_get_list_status_result_size=4, op_open=0, gcs_api_client_unauthorized_response_count=0, action_http_head_request_failures=0, stream_read_close_operations=0, stream_read_bytes_backwards_on_seek=0, exception_count=17, gcs_api_total_request_count=24, op_create=1, gcs_api_client_bad_request_count=0, op_create_non_recursive=0, gcs_api_client_gone_response_count=0, stream_write_operations=0, stream_read_operations=0, gcs_api_client_request_timeout_count=0, op_rename=0, op_get_file_status=2, stream_read_total_bytes=0, op_glob_status=1, stream_read_exceptions=0, action_http_get_request_failures=0, op_exists=0, stream_write_bytes=115371, op_xattr_list=0, stream_write_exceptions=0, gcs_api_server_unavailable_count=0, directories_created=0, files_delete_rejected=0, op_xattr_get_named=0, op_hsync=0, stream_read_operations_incomplete=0, op_delete=0, stream_read_bytes=0, gcs_api_client_non_found_response_count=8, gcs_api_client_requested_range_not_statisfiable_count=0, op_hflush=0, op_list_status=5, op_xattr_get_named_map=0, gcs_api_client_side_error_count=24, op_get_file_checksum=0, action_http_delete_request_failures=0, gcs_api_server_internal_error_count=0, stream_read_seek_bytes_skipped=0, stream_write_close_operations=0, op_list_files=0, files_deleted=0, op_mkdirs=1, gcs_api_client_rate_limit_error_count=0, action_http_put_request_failures=0, gcs_api_server_bad_gateway_count=0, stream_read_seek_backward_operations=0, gcs_api_server_side_error_count=0, action_http_patch_request_failures=0, stream_read_seek_operations=0, stream_read_seek_forward_operations=0, gcs_api_client_precondition_failed_response_count=1, directories_deleted=0, op_xattr_get_map=0, delegation_tokens_issued=0, op_create_min=163, op_delete_min=0, op_mkdirs_min=287, op_create_non_recursive_min=0, op_glob_status_min=209, op_hsync_min=0, op_xattr_get_named_min=0, op_list_status_min=32, op_xattr_get_named_map_min=0, stream_read_close_operations_min=0, stream_read_operations_min=0, stream_read_seek_operations_min=0, op_hflush_min=0, op_xattr_get_map_min=0, op_xattr_list_min=0, stream_write_operations_min=0, op_get_file_status_min=117, op_open_min=0, op_rename_min=0, delegation_tokens_issued_min=0, stream_write_close_operations_min=0, stream_read_close_operations_max=0, stream_read_operations_max=0, stream_read_seek_operations_max=0, op_hflush_max=0, op_xattr_list_max=0, op_xattr_get_map_max=0, op_xattr_get_named_max=0, op_create_non_recursive_max=0, op_glob_status_max=209, op_get_file_status_max=517, stream_write_close_operations_max=0, op_open_max=0, delegation_tokens_issued_max=0, op_mkdirs_max=287, op_rename_max=0, op_create_max=163, op_delete_max=0, op_list_status_max=57, op_xattr_get_named_map_max=0, stream_write_operations_max=0, op_hsync_max=0, op_list_status_mean=41, stream_read_close_operations_mean=0, op_open_mean=0, op_xattr_get_named_map_mean=0, op_xattr_list_mean=0, op_mkdirs_mean=287, stream_write_close_operations_mean=0, op_rename_mean=0, op_hsync_mean=0, delegation_tokens_issued_mean=0, stream_read_operations_mean=0, op_xattr_get_map_mean=0, op_create_mean=163, op_glob_status_mean=209, op_delete_mean=0, stream_read_seek_operations_mean=0, stream_write_operations_mean=0, op_create_non_recursive_mean=0, op_hflush_mean=0, op_xattr_get_named_mean=0, op_get_file_status_mean=317, stream_write_operations_duration=0, stream_read_operations_duration=0]
ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [13c44a9dc0e54e499f37bf7de23d0224] failed with error:
Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found at:
https://console.cloud.google.com/dataproc/jobs/13c44a9dc0e54e499f37bf7de23d0224?project=ithaproj1&region=us-central1
gcloud dataproc jobs wait '13c44a9dc0e54e499f37bf7de23d0224' --region 'us-central1' --project 'ithaproj1'
https://console.cloud.google.com/storage/browser/dataproc-staging-us-central1-746581037773-yalthsq2/google-cloud-dataproc-metainfo/4b454d3c-eb6c-4539-ad8d-85da9186474b/jobs/13c44a9dc0e54e499f37bf7de23d0224/
gs://dataproc-staging-us-central1-746581037773-yalthsq2/google-cloud-dataproc-metainfo/4b454d3c-eb6c-4539-ad8d-85da9186474b/jobs/13c44a9dc0e54e499f37bf7de23d0224/driveroutput.*

C:\Users\saisr\AppData\Local\Google\Cloud SDK>gcloud dataproc jobs submit pyspark gs:// --cluster=cluster-name --region=us-central1

C:\Users\saisr\AppData\Local\Google\Cloud SDK>gcloud dataproc jobs submit pyspark gs://itha-bucket/data/dataproc_failed_filter_to_mysql.py --cluster=cluster-name --region=us-central1
Job [cf50f281417942eb95b95fbb9808707e] submitted.
Waiting for job output...
25/05/08 16:47:58 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
25/05/08 16:47:58 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
25/05/08 16:47:58 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat
25/05/08 16:47:58 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
25/05/08 16:47:59 INFO org.sparkproject.jetty.util.log: Logging initialized @4482ms to org.sparkproject.jetty.util.log.Slf4jLog
25/05/08 16:47:59 INFO org.sparkproject.jetty.server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_452-b09
25/05/08 16:47:59 INFO org.sparkproject.jetty.server.Server: Started @4663ms
25/05/08 16:47:59 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@6a629e0a{HTTP/1.1, (http/1.1)}{0.0.0.0:39969}
25/05/08 16:48:01 INFO com.google.cloud.dataproc.DataprocSparkPlugin: Registered 128 driver metrics
25/05/08 16:48:02 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at cluster-name-m/10.128.0.14:8032
25/05/08 16:48:02 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at cluster-name-m/10.128.0.14:10200
25/05/08 16:48:03 INFO org.apache.hadoop.conf.Configuration: resource-types.xml not found
25/05/08 16:48:03 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Unable to find 'resource-types.xml'.
25/05/08 16:48:04 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1746720119508_0006
25/05/08 16:48:05 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at cluster-name-m/10.128.0.14:8030
25/05/08 16:48:07 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
Traceback (most recent call last):
  File "/tmp/cf50f281417942eb95b95fbb9808707e/dataproc_failed_filter_to_mysql.py", line 19, in <module>
    df_failed.write.jdbc(url=jdbc_url, table="failed_transactions", mode="overwrite", properties=properties)
  File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 1445, in jdbc
  File "/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1304, in __call__
  File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 111, in deco
  File "/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py", line 326, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o74.jdbc.
: java.sql.SQLException: Access denied for user 'admin'@'34.31.216.204' (using password: YES)
        at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:129)
        at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)
        at com.mysql.cj.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:824)
        at com.mysql.cj.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:444)
        at com.mysql.cj.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:237)
        at com.mysql.cj.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:198)
        at org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)
        at org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProvider$.create(ConnectionProvider.scala:77)
        at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$createConnectionFactory$1(JdbcUtils.scala:62)
        at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:48)
        at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)
        at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
        at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
        at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
        at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:133)
        at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:132)
        at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
        at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)
        at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)
        at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)
        at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:817)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:238)
        at java.lang.Thread.run(Thread.java:750)

25/05/08 16:48:34 INFO org.sparkproject.jetty.server.AbstractConnector: Stopped Spark@6a629e0a{HTTP/1.1, (http/1.1)}{0.0.0.0:0}
25/05/08 16:48:34 INFO com.google.cloud.dataproc.DataprocSparkPlugin: Shutting down driver plugin. metrics=[files_created=1, gcs_api_server_not_implemented_error_count=0, gcs_api_server_timeout_count=0, action_http_post_request_failures=0, op_get_list_status_result_size=7, op_open=0, gcs_api_client_unauthorized_response_count=0, action_http_head_request_failures=0, stream_read_close_operations=0, stream_read_bytes_backwards_on_seek=0, exception_count=16, gcs_api_total_request_count=24, op_create=1, gcs_api_client_bad_request_count=0, op_create_non_recursive=0, gcs_api_client_gone_response_count=0, stream_write_operations=0, stream_read_operations=0, gcs_api_client_request_timeout_count=0, op_rename=0, op_get_file_status=2, stream_read_total_bytes=0, op_glob_status=1, stream_read_exceptions=0, action_http_get_request_failures=0, op_exists=0, stream_write_bytes=115372, op_xattr_list=0, stream_write_exceptions=0, gcs_api_server_unavailable_count=0, directories_created=0, files_delete_rejected=0, op_xattr_get_named=0, op_hsync=0, stream_read_operations_incomplete=0, op_delete=0, stream_read_bytes=0, gcs_api_client_non_found_response_count=8, gcs_api_client_requested_range_not_statisfiable_count=0, op_hflush=0, op_list_status=5, op_xattr_get_named_map=0, gcs_api_client_side_error_count=24, op_get_file_checksum=0, action_http_delete_request_failures=0, gcs_api_server_internal_error_count=0, stream_read_seek_bytes_skipped=0, stream_write_close_operations=0, op_list_files=0, files_deleted=0, op_mkdirs=1, gcs_api_client_rate_limit_error_count=0, action_http_put_request_failures=0, gcs_api_server_bad_gateway_count=0, stream_read_seek_backward_operations=0, gcs_api_server_side_error_count=0, action_http_patch_request_failures=0, stream_read_seek_operations=0, stream_read_seek_forward_operations=0, gcs_api_client_precondition_failed_response_count=1, directories_deleted=0, op_xattr_get_map=0, delegation_tokens_issued=0, op_create_min=113, op_delete_min=0, op_mkdirs_min=234, op_create_non_recursive_min=0, op_glob_status_min=244, op_hsync_min=0, op_xattr_get_named_min=0, op_list_status_min=30, op_xattr_get_named_map_min=0, stream_read_close_operations_min=0, stream_read_operations_min=0, stream_read_seek_operations_min=0, op_hflush_min=0, op_xattr_get_map_min=0, op_xattr_list_min=0, stream_write_operations_min=0, op_get_file_status_min=126, op_open_min=0, op_rename_min=0, delegation_tokens_issued_min=0, stream_write_close_operations_min=0, stream_read_close_operations_max=0, stream_read_operations_max=0, stream_read_seek_operations_max=0, op_hflush_max=0, op_xattr_list_max=0, op_xattr_get_map_max=0, op_xattr_get_named_max=0, op_create_non_recursive_max=0, op_glob_status_max=244, op_get_file_status_max=346, stream_write_close_operations_max=0, op_open_max=0, delegation_tokens_issued_max=0, op_mkdirs_max=234, op_rename_max=0, op_create_max=113, op_delete_max=0, op_list_status_max=60, op_xattr_get_named_map_max=0, stream_write_operations_max=0, op_hsync_max=0, op_list_status_mean=40, stream_read_close_operations_mean=0, op_open_mean=0, op_xattr_get_named_map_mean=0, op_xattr_list_mean=0, op_mkdirs_mean=234, stream_write_close_operations_mean=0, op_rename_mean=0, op_hsync_mean=0, delegation_tokens_issued_mean=0, stream_read_operations_mean=0, op_xattr_get_map_mean=0, op_create_mean=113, op_glob_status_mean=244, op_delete_mean=0, stream_read_seek_operations_mean=0, stream_write_operations_mean=0, op_create_non_recursive_mean=0, op_hflush_mean=0, op_xattr_get_named_mean=0, op_get_file_status_mean=236, stream_write_operations_duration=0, stream_read_operations_duration=0]
ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [cf50f281417942eb95b95fbb9808707e] failed with error:
Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found at:
https://console.cloud.google.com/dataproc/jobs/cf50f281417942eb95b95fbb9808707e?project=ithaproj1&region=us-central1
gcloud dataproc jobs wait 'cf50f281417942eb95b95fbb9808707e' --region 'us-central1' --project 'ithaproj1'
https://console.cloud.google.com/storage/browser/dataproc-staging-us-central1-746581037773-yalthsq2/google-cloud-dataproc-metainfo/4b454d3c-eb6c-4539-ad8d-85da9186474b/jobs/cf50f281417942eb95b95fbb9808707e/
gs://dataproc-staging-us-central1-746581037773-yalthsq2/google-cloud-dataproc-metainfo/4b454d3c-eb6c-4539-ad8d-85da9186474b/jobs/cf50f281417942eb95b95fbb9808707e/driveroutput.*

C:\Users\saisr\AppData\Local\Google\Cloud SDK>gsutil cp "C:\Users\saisr\OneDrive\Desktop\project1\dataproc_failed_filter_to_mysql.py" gs://itha-bucket/data/
Copying file://C:\Users\saisr\OneDrive\Desktop\project1\dataproc_failed_filter_to_mysql.py [Content-Type=text/x-python]...
\ [1 files][  649.0 B/  649.0 B]
Operation completed over 1 objects/649.0 B.

C:\Users\saisr\AppData\Local\Google\Cloud SDK>gcloud dataproc jobs submit pyspark gs://itha-bucket/data/dataproc_failed_filter_to_mysql.py --cluster=cluster-name --region=us-central1
Job [8017dbf429b246bfa642dfb18722c81b] submitted.
Waiting for job output...
25/05/08 16:54:07 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
25/05/08 16:54:07 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
25/05/08 16:54:07 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat
25/05/08 16:54:07 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
25/05/08 16:54:07 INFO org.sparkproject.jetty.util.log: Logging initialized @4672ms to org.sparkproject.jetty.util.log.Slf4jLog
25/05/08 16:54:07 INFO org.sparkproject.jetty.server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_452-b09
25/05/08 16:54:07 INFO org.sparkproject.jetty.server.Server: Started @4834ms
25/05/08 16:54:07 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@2088a874{HTTP/1.1, (http/1.1)}{0.0.0.0:43235}
25/05/08 16:54:09 INFO com.google.cloud.dataproc.DataprocSparkPlugin: Registered 128 driver metrics
25/05/08 16:54:10 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at cluster-name-m/10.128.0.14:8032
25/05/08 16:54:11 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at cluster-name-m/10.128.0.14:10200
25/05/08 16:54:11 INFO org.apache.hadoop.conf.Configuration: resource-types.xml not found
25/05/08 16:54:11 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Unable to find 'resource-types.xml'.
25/05/08 16:54:12 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1746720119508_0007
25/05/08 16:54:13 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at cluster-name-m/10.128.0.14:8030
25/05/08 16:54:15 INFO com.google.cloud.hadoop.fs.gcs.GhfsStorageStatistics: Detected potential high latency for operation op_get_file_status. latencyMs=528; previousMaxLatencyMs=0; operationCount=1; context=gs://dataproc-temp-us-central1-746581037773-8zj5ivyl/4b454d3c-eb6c-4539-ad8d-85da9186474b/spark-job-history
25/05/08 16:54:15 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
25/05/08 16:54:53 INFO org.sparkproject.jetty.server.AbstractConnector: Stopped Spark@2088a874{HTTP/1.1, (http/1.1)}{0.0.0.0:0}
25/05/08 16:54:54 INFO com.google.cloud.dataproc.DataprocSparkPlugin: Shutting down driver plugin. metrics=[files_created=1, gcs_api_server_not_implemented_error_count=0, gcs_api_server_timeout_count=0, action_http_post_request_failures=0, op_get_list_status_result_size=7, op_open=0, gcs_api_client_unauthorized_response_count=0, action_http_head_request_failures=0, stream_read_close_operations=0, stream_read_bytes_backwards_on_seek=0, exception_count=16, gcs_api_total_request_count=24, op_create=1, gcs_api_client_bad_request_count=0, op_create_non_recursive=0, gcs_api_client_gone_response_count=0, stream_write_operations=0, stream_read_operations=0, gcs_api_client_request_timeout_count=0, op_rename=0, op_get_file_status=2, stream_read_total_bytes=0, op_glob_status=1, stream_read_exceptions=0, action_http_get_request_failures=0, op_exists=0, stream_write_bytes=140120, op_xattr_list=0, stream_write_exceptions=0, gcs_api_server_unavailable_count=0, directories_created=0, files_delete_rejected=0, op_xattr_get_named=0, op_hsync=0, stream_read_operations_incomplete=0, op_delete=0, stream_read_bytes=0, gcs_api_client_non_found_response_count=8, gcs_api_client_requested_range_not_statisfiable_count=0, op_hflush=0, op_list_status=5, op_xattr_get_named_map=0, gcs_api_client_side_error_count=24, op_get_file_checksum=0, action_http_delete_request_failures=0, gcs_api_server_internal_error_count=0, stream_read_seek_bytes_skipped=0, stream_write_close_operations=0, op_list_files=0, files_deleted=0, op_mkdirs=1, gcs_api_client_rate_limit_error_count=0, action_http_put_request_failures=0, gcs_api_server_bad_gateway_count=0, stream_read_seek_backward_operations=0, gcs_api_server_side_error_count=0, action_http_patch_request_failures=0, stream_read_seek_operations=0, stream_read_seek_forward_operations=0, gcs_api_client_precondition_failed_response_count=1, directories_deleted=0, op_xattr_get_map=0, delegation_tokens_issued=0, op_create_min=184, op_delete_min=0, op_mkdirs_min=281, op_create_non_recursive_min=0, op_glob_status_min=208, op_hsync_min=0, op_xattr_get_named_min=0, op_list_status_min=32, op_xattr_get_named_map_min=0, stream_read_close_operations_min=0, stream_read_operations_min=0, stream_read_seek_operations_min=0, op_hflush_min=0, op_xattr_get_map_min=0, op_xattr_list_min=0, stream_write_operations_min=0, op_get_file_status_min=98, op_open_min=0, op_rename_min=0, delegation_tokens_issued_min=0, stream_write_close_operations_min=0, stream_read_close_operations_max=0, stream_read_operations_max=0, stream_read_seek_operations_max=0, op_hflush_max=0, op_xattr_list_max=0, op_xattr_get_map_max=0, op_xattr_get_named_max=0, op_create_non_recursive_max=0, op_glob_status_max=208, op_get_file_status_max=528, stream_write_close_operations_max=0, op_open_max=0, delegation_tokens_issued_max=0, op_mkdirs_max=281, op_rename_max=0, op_create_max=184, op_delete_max=0, op_list_status_max=84, op_xattr_get_named_map_max=0, stream_write_operations_max=0, op_hsync_max=0, op_list_status_mean=51, stream_read_close_operations_mean=0, op_open_mean=0, op_xattr_get_named_map_mean=0, op_xattr_list_mean=0, op_mkdirs_mean=281, stream_write_close_operations_mean=0, op_rename_mean=0, op_hsync_mean=0, delegation_tokens_issued_mean=0, stream_read_operations_mean=0, op_xattr_get_map_mean=0, op_create_mean=184, op_glob_status_mean=208, op_delete_mean=0, stream_read_seek_operations_mean=0, stream_write_operations_mean=0, op_create_non_recursive_mean=0, op_hflush_mean=0, op_xattr_get_named_mean=0, op_get_file_status_mean=313, stream_write_operations_duration=0, stream_read_operations_duration=0]
Job [8017dbf429b246bfa642dfb18722c81b] finished successfully.
done: true
driverControlFilesUri: gs://dataproc-staging-us-central1-746581037773-yalthsq2/google-cloud-dataproc-metainfo/4b454d3c-eb6c-4539-ad8d-85da9186474b/jobs/8017dbf429b246bfa642dfb18722c81b/
driverOutputResourceUri: gs://dataproc-staging-us-central1-746581037773-yalthsq2/google-cloud-dataproc-metainfo/4b454d3c-eb6c-4539-ad8d-85da9186474b/jobs/8017dbf429b246bfa642dfb18722c81b/driveroutput
jobUuid: a854cafe-b2d1-344d-968f-41debf7925de
placement:
  clusterName: cluster-name
  clusterUuid: 4b454d3c-eb6c-4539-ad8d-85da9186474b
pysparkJob:
  mainPythonFileUri: gs://itha-bucket/data/dataproc_failed_filter_to_mysql.py
reference:
  jobId: 8017dbf429b246bfa642dfb18722c81b
  projectId: ithaproj1
status:
  state: DONE
  stateStartTime: '2025-05-08T16:54:56.039114Z'
statusHistory:
- state: PENDING
  stateStartTime: '2025-05-08T16:54:01.360105Z'
- state: SETUP_DONE
  stateStartTime: '2025-05-08T16:54:01.383203Z'
- details: Agent reported job success
  state: RUNNING
  stateStartTime: '2025-05-08T16:54:01.668903Z'
yarnApplications:
- name: FilterFailedTransactions
  progress: 1.0
  state: FINISHED
  trackingUrl: http://cluster-name-m:8088/proxy/application_1746720119508_0007/

C:\Users\saisr\AppData\Local\Google\Cloud SDK>